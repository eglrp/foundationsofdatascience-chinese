\chapter{引言}
计算机科学作为一门学科出现是在上世纪六十年代.那时的研究重点是编程语言、编译器、操作系统和与之相关的数学理论.理论计算机科学课程的内容包括有限状态自动机、正则表达式、上下文无关语言和可计算性.上世纪七十年代，算法也被作为理论的重要组成部分加入.研究的重点是如何让计算机变得有用.当下，一场颠覆性的变革正在发生，我们更加关注计算机科学的应用.这一变化有许多原因.计算与通信的交融是一个重要原因.我们在自然科学、经济和其他领域中观察采集和存储数据的能力变得更强了，这就要求我们改变我们对数据的传统认知，考虑在现代环境中如何对其进行处理.成为我们生活中心的互联网和社交网络，既是理论的机遇，也是对理论的挑战.

%原书：increasingly [many] researchers
计算机科学的传统领域仍然非常重要，但未来会有更多的科研工作者需要利用计算机来理解实际应用中收集到的海量数据，并从中提取有用的信息，而不仅仅是让计算机在具体明确的问题上发挥作用.基于这样的想法，我们编写了这本教材，它包含了未来$40$年可能比较有用的理论，就像过去$40$年里让学生受益的自动机理论、算法及其他相关的问题一样.最大的变化之一是侧重点从离散数学更多地转向概率论、统计学和数值计算方法.

这本书的早期草稿被用于本科生和研究生课程.本科生所需要的背景知识见附录.我们为此还在附录里安排了习题.

本书从高维几何开始讲起.信息处理、搜索、\gls{ML}等不同领域中的现代数据经常可以方便地表示为具有很多分量的向量，即便向量表示并非直观首选.到了高维空间中，我们从二维和三维空间得来的直觉常常错得离谱.第~\ref{chapter:2}~章主要讨论了这样的差异.这一章的重点——同时也是这本书的重点——是讲清数学基础.具体的应用我们只作简要描述.

与高维数据关系最密切的数学领域是矩阵代数和算法.我们关注\gls{SVD}，这是这部分的核心工具.第~\ref{chapter:3}~章从原理出发作了详细的介绍.\gls{SVD}的应用包括我们将要简要提及的\gls{PCA}，以及统计学中概率密度的混合模型的现代应用、离散最优化等等，这些我们将详细讨论.

要理解互联网和社交网络这样的大型结构，核心是抓住这些结构的关键性质进行建模.最简单的模型是由Erd\"os和R\'enyi提出的\gls{random_graph}，我们之后将会对其进行研究，证明这种结构中出现的某些全局现象只有局部选择，例如\gls{giant_component}.我们还会描述其它\gls{random_graph}模型.

与图结构关系紧密的是在这些结构上进行的\gls{random_walk}.\gls{random_walk}的\gls{stationary_distribution}在从网络搜索到物理系统模拟的各种应用中都很重要.\gls{random_walk}背后的数学理论以及它与\gls{electrical_network}间的联系是\gls{markov_chain}这一章的核心内容.

计算机科学在过去二十年里最惊人的成就是用一些领域无关的方法成功解决了许多不同领域中的问题.\gls{ML}就是一个显著的例子.我们介绍\gls{ML}的基石——对给定的训练样本做优化，以及用来理解这些优化技术能对新的、未见过的数据有效原因的理论，例如\gls{VC}理论等重要的方法.

传统的算法领域假定问题的输入存放于随机访问存储器，算法可以反复访问这些数据.在现代问题中这是不可行的.流式模型等模型的出现就是为了更好地研究这一问题.这一背景下，采样扮演了重要角色，事实上，我们需要进行\cleangls{on_the_fly}采样.第~\ref{chapter:6}~章我们研究如何高效取出较好的样本，以及如何用这些样本估计统计量与线性代数中的量.

理解数据的另一重要工具是\gls{clustering}——将数据分成若干组具有相同特点的对象.在描述了\gls{k_means}算法等\gls{clustering}的基本方法后，我们将关注对这些算法和新算法的现代理解，并分析不同\gls{clustering}问题的一般框架.

本书还包括\gls{graphical_model}和\gls{belief_propagation}、\gls{ranking}、\gls{sparse_vector}以及\gls{compressed_sensing}.我们还会讨论\gls{wavelet}和它的一些基本数学性质，它是许多应用中表示信号的重要方法.附录中包含大量背景材料.

关于本书所用的记号：为了照顾学生，我们采用了一些固定的符号，有少数例外.我们用小写字母表示数值变量和函数，粗体小写字母表示向量，大写字母表示矩阵.字母表开头的小写字母通常表示常数；字母表中部的字母如$i,j,k$用作求和的下标，$n,m$用于表示整数值的大小，$x,y,z$表示变量.矩阵$A$的元素是$a_{ij}$，各行为$\rva_i$.如果$\rva_i$是一个向量，那么它的各坐标为$a_{ij}$.如果其他文献有用某个符号表示某个量的传统，我们也会沿用，即使这与我们的约定相悖.如果我们遇到向量空间中的一个点集，需要讨论某个子空间，则用$n$代表数据点的个数，$d$代表空间的维数，$k$代表子空间的维数.

术语“几乎必然”表示概率为$1$.我们以$\ln n$记自然对数，以$\log n$表示底为$2$的对数.如果我们想表示底为$10$的对数，我们会用$\log_{10}$.为简化标记使之更易于阅读.我们用$E^2(1-x)$代表$\big( E(1-x)\big)^2$,用$E(1-x)^2$代表$E\big((1-x)\big)^2$.从给定的概率分布中“随机选取”一定量的点时，如无特别说明，我们总假定选取时的独立性.
\cleardoublepage